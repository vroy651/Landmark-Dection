{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8261424,"sourceType":"datasetVersion","datasetId":4903497},{"sourceId":8265323,"sourceType":"datasetVersion","datasetId":4906462}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the required libraries\nfrom torchvision import datasets,transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:46:46.509535Z","iopub.execute_input":"2024-05-03T14:46:46.509933Z","iopub.status.idle":"2024-05-03T14:46:46.826824Z","shell.execute_reply.started":"2024-05-03T14:46:46.509899Z","shell.execute_reply":"2024-05-03T14:46:46.826028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# define device to use the gpu\ndevice = 'cuda' if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:45:31.441446Z","iopub.execute_input":"2024-05-03T14:45:31.442193Z","iopub.status.idle":"2024-05-03T14:45:31.495942Z","shell.execute_reply.started":"2024-05-03T14:45:31.442155Z","shell.execute_reply":"2024-05-03T14:45:31.494799Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# image directory\nimage_dir='/kaggle/input/mydataset/images'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:45:45.344611Z","iopub.execute_input":"2024-05-03T14:45:45.345035Z","iopub.status.idle":"2024-05-03T14:45:45.349436Z","shell.execute_reply.started":"2024-05-03T14:45:45.345001Z","shell.execute_reply":"2024-05-03T14:45:45.348430Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ocnvert the image directory to image list containing the image name example : image.png\nimage_filenames=os.listdir(image_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:46:33.147288Z","iopub.execute_input":"2024-05-03T14:46:33.147986Z","iopub.status.idle":"2024-05-03T14:46:33.152795Z","shell.execute_reply.started":"2024-05-03T14:46:33.147951Z","shell.execute_reply":"2024-05-03T14:46:33.151811Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# define the list of all images\nimages = [Image.open(os.path.join(image_dir, filename)) for filename in image_filenames]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:47:15.304904Z","iopub.execute_input":"2024-05-03T14:47:15.305441Z","iopub.status.idle":"2024-05-03T14:47:20.052767Z","shell.execute_reply.started":"2024-05-03T14:47:15.305410Z","shell.execute_reply":"2024-05-03T14:47:20.051715Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# number of data points or images in images list\nlen(images)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:47:46.838384Z","iopub.execute_input":"2024-05-03T14:47:46.838781Z","iopub.status.idle":"2024-05-03T14:47:46.847035Z","shell.execute_reply.started":"2024-05-03T14:47:46.838730Z","shell.execute_reply":"2024-05-03T14:47:46.846008Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"622"},"metadata":{}}]},{"cell_type":"code","source":"# images = torch.stack([preprocess(image) for image in images])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ocnvert the annotation of image into pandas's dataframe\ndf=pd.read_csv('/kaggle/input/label-csv/role_challenge_dataset_ground_truth.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:48:20.166434Z","iopub.execute_input":"2024-05-03T14:48:20.167359Z","iopub.status.idle":"2024-05-03T14:48:20.185576Z","shell.execute_reply.started":"2024-05-03T14:48:20.167323Z","shell.execute_reply":"2024-05-03T14:48:20.184610Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# first five example of corresponding image name labels\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:48:49.356804Z","iopub.execute_input":"2024-05-03T14:48:49.357469Z","iopub.status.idle":"2024-05-03T14:48:49.376020Z","shell.execute_reply.started":"2024-05-03T14:48:49.357437Z","shell.execute_reply":"2024-05-03T14:48:49.375078Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   image_name  ofd_1_x  ofd_1_y  ofd_2_x  ofd_2_y  bpd_1_x  bpd_1_y  bpd_2_x  \\\n0  000_HC.png      361       12      339      530      481       16      664   \n1  001_HC.png      441      331      368      308      297      247      534   \n2  002_HC.png      318      374      154      406      481      158      558   \n3  003_HC.png      424      105      407      462      305      349      547   \n4  004_HC.png      300      277      611      534       53      452      494   \n\n   bpd_2_y  \n0      318  \n1      142  \n2      215  \n3      363  \n4      308  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>ofd_1_x</th>\n      <th>ofd_1_y</th>\n      <th>ofd_2_x</th>\n      <th>ofd_2_y</th>\n      <th>bpd_1_x</th>\n      <th>bpd_1_y</th>\n      <th>bpd_2_x</th>\n      <th>bpd_2_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000_HC.png</td>\n      <td>361</td>\n      <td>12</td>\n      <td>339</td>\n      <td>530</td>\n      <td>481</td>\n      <td>16</td>\n      <td>664</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001_HC.png</td>\n      <td>441</td>\n      <td>331</td>\n      <td>368</td>\n      <td>308</td>\n      <td>297</td>\n      <td>247</td>\n      <td>534</td>\n      <td>142</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>002_HC.png</td>\n      <td>318</td>\n      <td>374</td>\n      <td>154</td>\n      <td>406</td>\n      <td>481</td>\n      <td>158</td>\n      <td>558</td>\n      <td>215</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003_HC.png</td>\n      <td>424</td>\n      <td>105</td>\n      <td>407</td>\n      <td>462</td>\n      <td>305</td>\n      <td>349</td>\n      <td>547</td>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>004_HC.png</td>\n      <td>300</td>\n      <td>277</td>\n      <td>611</td>\n      <td>534</td>\n      <td>53</td>\n      <td>452</td>\n      <td>494</td>\n      <td>308</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"for _,row in df.iterrows():\n    pass\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of columns contain the annotation file\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:49:16.705566Z","iopub.execute_input":"2024-05-03T14:49:16.705968Z","iopub.status.idle":"2024-05-03T14:49:16.712727Z","shell.execute_reply.started":"2024-05-03T14:49:16.705937Z","shell.execute_reply":"2024-05-03T14:49:16.711687Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Index(['image_name', 'ofd_1_x', 'ofd_1_y', 'ofd_2_x', 'ofd_2_y', 'bpd_1_x',\n       'bpd_1_y', 'bpd_2_x', 'bpd_2_y'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"# map the image name to its coressponding lebels\nimage_landmark_map = {row['image_name']: [row['ofd_1_x'], row['ofd_1_y'],row['ofd_2_x'],row['ofd_2_y'],row['bpd_1_x'],row['bpd_1_y'],row['bpd_2_x'],row['bpd_2_y']] for _, row in df.iterrows()}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:50:02.635958Z","iopub.execute_input":"2024-05-03T14:50:02.636327Z","iopub.status.idle":"2024-05-03T14:50:02.702001Z","shell.execute_reply.started":"2024-05-03T14:50:02.636297Z","shell.execute_reply":"2024-05-03T14:50:02.701193Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# combine the images and its labels\nformatted_data = [(image, image_landmark_map[filename]) for image, filename in zip(images, image_filenames)]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:50:28.105123Z","iopub.execute_input":"2024-05-03T14:50:28.105495Z","iopub.status.idle":"2024-05-03T14:50:28.110985Z","shell.execute_reply.started":"2024-05-03T14:50:28.105464Z","shell.execute_reply":"2024-05-03T14:50:28.109819Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# example of image and its coresponding cordinates\nprint(f\"image: {formatted_data[0][0]}\\n\")\nprint(f\"label: {formatted_data[0][1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:51:56.591300Z","iopub.execute_input":"2024-05-03T14:51:56.591683Z","iopub.status.idle":"2024-05-03T14:51:56.597122Z","shell.execute_reply.started":"2024-05-03T14:51:56.591654Z","shell.execute_reply":"2024-05-03T14:51:56.596163Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"image: <PIL.PngImagePlugin.PngImageFile image mode=L size=800x540 at 0x787CDEE3CE80>\n\nlabel: [371, 70, 309, 451, 115, 273, 642, 393]\n","output_type":"stream"}]},{"cell_type":"code","source":"# convert the label data frame into numpy array\nlabel=(np.array(df))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:52:44.315886Z","iopub.execute_input":"2024-05-03T14:52:44.316263Z","iopub.status.idle":"2024-05-03T14:52:44.321379Z","shell.execute_reply.started":"2024-05-03T14:52:44.316232Z","shell.execute_reply":"2024-05-03T14:52:44.320272Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"label","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:52:50.338720Z","iopub.execute_input":"2024-05-03T14:52:50.339112Z","iopub.status.idle":"2024-05-03T14:52:50.345816Z","shell.execute_reply.started":"2024-05-03T14:52:50.339081Z","shell.execute_reply":"2024-05-03T14:52:50.344783Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([['000_HC.png', 361, 12, ..., 16, 664, 318],\n       ['001_HC.png', 441, 331, ..., 247, 534, 142],\n       ['002_HC.png', 318, 374, ..., 158, 558, 215],\n       ...,\n       ['499_2HC.png', 432, 105, ..., 284, 647, 296],\n       ['499_HC.png', 356, 119, ..., 347, 608, 272],\n       ['500_HC.png', 429, 70, ..., 258, 652, 213]], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# Define transformations for augmentation of image\npreprocess = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:53:18.002681Z","iopub.execute_input":"2024-05-03T14:53:18.003068Z","iopub.status.idle":"2024-05-03T14:53:18.008511Z","shell.execute_reply.started":"2024-05-03T14:53:18.003038Z","shell.execute_reply":"2024-05-03T14:53:18.007534Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# difine a list of containing the label\nlandmarks=[label for _,label in formatted_data]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:53:48.115869Z","iopub.execute_input":"2024-05-03T14:53:48.116620Z","iopub.status.idle":"2024-05-03T14:53:48.121135Z","shell.execute_reply.started":"2024-05-03T14:53:48.116587Z","shell.execute_reply":"2024-05-03T14:53:48.119998Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"len(landmarks[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:53:51.686822Z","iopub.execute_input":"2024-05-03T14:53:51.687191Z","iopub.status.idle":"2024-05-03T14:53:51.693764Z","shell.execute_reply.started":"2024-05-03T14:53:51.687162Z","shell.execute_reply":"2024-05-03T14:53:51.692713Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"code","source":"len(landmarks)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:53:57.773591Z","iopub.execute_input":"2024-05-03T14:53:57.774321Z","iopub.status.idle":"2024-05-03T14:53:57.780354Z","shell.execute_reply.started":"2024-05-03T14:53:57.774287Z","shell.execute_reply":"2024-05-03T14:53:57.779328Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"622"},"metadata":{}}]},{"cell_type":"code","source":"transform=transforms.Compose([\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# landmarks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the landmarks into tensor format \nlandmarks_tensor = torch.tensor(landmarks,dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:54:22.436691Z","iopub.execute_input":"2024-05-03T14:54:22.437554Z","iopub.status.idle":"2024-05-03T14:54:22.452439Z","shell.execute_reply.started":"2024-05-03T14:54:22.437503Z","shell.execute_reply":"2024-05-03T14:54:22.451391Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"landmarks_tensor[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:54:32.345963Z","iopub.execute_input":"2024-05-03T14:54:32.346338Z","iopub.status.idle":"2024-05-03T14:54:32.412114Z","shell.execute_reply.started":"2024-05-03T14:54:32.346307Z","shell.execute_reply":"2024-05-03T14:54:32.411158Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([371.,  70., 309., 451., 115., 273., 642., 393.])"},"metadata":{}}]},{"cell_type":"code","source":"landmarks_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:54:38.826567Z","iopub.execute_input":"2024-05-03T14:54:38.827243Z","iopub.status.idle":"2024-05-03T14:54:38.832988Z","shell.execute_reply.started":"2024-05-03T14:54:38.827208Z","shell.execute_reply":"2024-05-03T14:54:38.832079Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"torch.Size([622, 8])"},"metadata":{}}]},{"cell_type":"code","source":"# Dataloader to wrap the image and label into batches\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:55:16.775657Z","iopub.execute_input":"2024-05-03T14:55:16.776625Z","iopub.status.idle":"2024-05-03T14:55:16.780761Z","shell.execute_reply.started":"2024-05-03T14:55:16.776591Z","shell.execute_reply":"2024-05-03T14:55:16.779649Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# # define the second model\n# class model_2(nn.Module):\n#     def __init__(self,):\n#         super().__init__()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the image into the tensor format and stack them\nimages_tensor=torch.stack([preprocess(image) for image,_ in formatted_data])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:55:23.757444Z","iopub.execute_input":"2024-05-03T14:55:23.757854Z","iopub.status.idle":"2024-05-03T14:55:28.062813Z","shell.execute_reply.started":"2024-05-03T14:55:23.757823Z","shell.execute_reply":"2024-05-03T14:55:28.061982Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# define dataloader for image and landmark \nimage_dataloader=DataLoader(images_tensor,batch_size=16,shuffle=False)\nlandmarks_dataloader=DataLoader(landmarks_tensor,batch_size=16,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:56:21.038445Z","iopub.execute_input":"2024-05-03T14:56:21.039319Z","iopub.status.idle":"2024-05-03T14:56:21.044134Z","shell.execute_reply.started":"2024-05-03T14:56:21.039287Z","shell.execute_reply":"2024-05-03T14:56:21.043095Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:56:27.939304Z","iopub.execute_input":"2024-05-03T14:56:27.939678Z","iopub.status.idle":"2024-05-03T14:56:27.944134Z","shell.execute_reply.started":"2024-05-03T14:56:27.939647Z","shell.execute_reply":"2024-05-03T14:56:27.943102Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"pretrained_resnet = models.resnet18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:56:32.335740Z","iopub.execute_input":"2024-05-03T14:56:32.336133Z","iopub.status.idle":"2024-05-03T14:56:33.047406Z","shell.execute_reply.started":"2024-05-03T14:56:32.336099Z","shell.execute_reply":"2024-05-03T14:56:33.046342Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 134MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:56:37.980859Z","iopub.execute_input":"2024-05-03T14:56:37.981494Z","iopub.status.idle":"2024-05-03T14:56:37.985769Z","shell.execute_reply.started":"2024-05-03T14:56:37.981462Z","shell.execute_reply":"2024-05-03T14:56:37.984676Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# change image channel in pretrained network for our image dataset\npretrained_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:57:13.342914Z","iopub.execute_input":"2024-05-03T14:57:13.343693Z","iopub.status.idle":"2024-05-03T14:57:13.348784Z","shell.execute_reply.started":"2024-05-03T14:57:13.343661Z","shell.execute_reply":"2024-05-03T14:57:13.347767Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# add the 8 neuron for our task\npretrained_resnet.fc = nn.Linear(pretrained_resnet.fc.in_features, 8)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:57:46.037378Z","iopub.execute_input":"2024-05-03T14:57:46.038119Z","iopub.status.idle":"2024-05-03T14:57:46.043345Z","shell.execute_reply.started":"2024-05-03T14:57:46.038083Z","shell.execute_reply":"2024-05-03T14:57:46.042148Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def image_to_tensor():\n    pass\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dataLoader(images,landmarks):\n#     images_tensor = torch.stack([preprocess(image)for image in images])\n#     image_dataloader=DataLoader(images_tensor,batch_size=16,shuffle=False)\n#     landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32)\n#     landmarks_dataloader=DataLoader(landmarks_tensor,batch_size=16,shuffle=False)\n#     return image_dataloader,landmarks_dataloader\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LandmarkDetectionModel(nn.Module):\n    def __init__(self):\n        super(LandmarkDetectionModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n        self.fc2 = nn.Linear(512, 8)  # Output: 8 coordinates for each landmark point\n\n    def forward(self, x):\n        x = self.pool(nn.functional.relu(self.conv1(x)))\n        x = self.pool(nn.functional.relu(self.conv2(x)))\n        x = self.pool(nn.functional.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 32 * 32)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:01:53.888740Z","iopub.execute_input":"2024-05-03T15:01:53.889161Z","iopub.status.idle":"2024-05-03T15:01:53.898992Z","shell.execute_reply.started":"2024-05-03T15:01:53.889132Z","shell.execute_reply":"2024-05-03T15:01:53.898002Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model=LandmarkDetectionModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:02:06.707596Z","iopub.execute_input":"2024-05-03T15:02:06.708466Z","iopub.status.idle":"2024-05-03T15:02:07.434354Z","shell.execute_reply.started":"2024-05-03T15:02:06.708430Z","shell.execute_reply":"2024-05-03T15:02:07.433260Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"output_1=model(images_tensor[1].to(device))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:02:10.969027Z","iopub.execute_input":"2024-05-03T15:02:10.969760Z","iopub.status.idle":"2024-05-03T15:02:10.975889Z","shell.execute_reply.started":"2024-05-03T15:02:10.969718Z","shell.execute_reply":"2024-05-03T15:02:10.974825Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"output_1","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:02:14.987013Z","iopub.execute_input":"2024-05-03T15:02:14.987831Z","iopub.status.idle":"2024-05-03T15:02:14.995884Z","shell.execute_reply.started":"2024-05-03T15:02:14.987795Z","shell.execute_reply":"2024-05-03T15:02:14.994820Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0486,  0.0009, -0.0558, -0.0384, -0.0215, -0.0150, -0.0214, -0.0173]],\n       device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"\n# define the model architecture \n\nclass LandmarkDetectionModel_2(nn.Module):\n    def __init__(self):\n        super(LandmarkDetectionModel_2, self).__init__()\n        \n        # Define convolutional layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        \n        # Define pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Define fully connected layers\n        self.fc1 = nn.Linear(512 * 8 * 8, 512)\n        self.fc2 = nn.Linear(512, 8)  # Output: 8 coordinates for each landmark point\n       \n        # Define dropout layer\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # Apply convolutional and pooling layers\n        x = self.pool(nn.functional.relu(self.conv1(x)))\n        x = self.pool(nn.functional.relu(self.conv2(x)))\n        x = self.pool(nn.functional.relu(self.conv3(x)))\n        x = self.pool(nn.functional.relu(self.conv4(x)))\n        x = self.pool(nn.functional.relu(self.conv5(x)))\n        \n        # Flatten the output for fully connected layers\n        x = x.view(-1, 512 * 8 * 8)\n        \n        # Apply fully connected layers with dropout\n        x = nn.functional.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:08:47.408225Z","iopub.execute_input":"2024-05-03T15:08:47.408918Z","iopub.status.idle":"2024-05-03T15:08:47.420059Z","shell.execute_reply.started":"2024-05-03T15:08:47.408883Z","shell.execute_reply":"2024-05-03T15:08:47.418990Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# intialize the model and shift it on device\nmodel_2=LandmarkDetectionModel_2().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:08:52.467471Z","iopub.execute_input":"2024-05-03T15:08:52.468204Z","iopub.status.idle":"2024-05-03T15:08:52.668179Z","shell.execute_reply.started":"2024-05-03T15:08:52.468170Z","shell.execute_reply":"2024-05-03T15:08:52.667300Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"output_2=model_2(images_tensor[1].to(device))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:08:56.591918Z","iopub.execute_input":"2024-05-03T15:08:56.592661Z","iopub.status.idle":"2024-05-03T15:08:56.617819Z","shell.execute_reply.started":"2024-05-03T15:08:56.592631Z","shell.execute_reply":"2024-05-03T15:08:56.617040Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"output_2","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:09:00.209989Z","iopub.execute_input":"2024-05-03T15:09:00.210790Z","iopub.status.idle":"2024-05-03T15:09:00.218687Z","shell.execute_reply.started":"2024-05-03T15:09:00.210759Z","shell.execute_reply":"2024-05-03T15:09:00.217562Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0242, -0.0118,  0.0418, -0.0294, -0.0398, -0.0396,  0.0439,  0.0433]],\n       device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the loss function\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super(RMSELoss, self).__init__()\n\n    def forward(self, y_true, y_pred):\n        mse = torch.mean((y_true - y_pred) ** 2)\n        rmse = torch.sqrt(mse)\n        return rmse\n\n# Create an instance of the RMSE loss\ncriterion = RMSELoss()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:00:58.408445Z","iopub.execute_input":"2024-05-03T15:00:58.409616Z","iopub.status.idle":"2024-05-03T15:00:58.417329Z","shell.execute_reply.started":"2024-05-03T15:00:58.409565Z","shell.execute_reply":"2024-05-03T15:00:58.415950Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# define loss function and optmizer\n# criterion=nn.MSELoss()\noptimizer=torch.optim.Adam(model.parameters(),lr=0.002)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:03:07.133680Z","iopub.execute_input":"2024-05-03T15:03:07.134339Z","iopub.status.idle":"2024-05-03T15:03:07.139051Z","shell.execute_reply.started":"2024-05-03T15:03:07.134307Z","shell.execute_reply":"2024-05-03T15:03:07.138084Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"images_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:03:11.544926Z","iopub.execute_input":"2024-05-03T15:03:11.545267Z","iopub.status.idle":"2024-05-03T15:03:11.551285Z","shell.execute_reply.started":"2024-05-03T15:03:11.545243Z","shell.execute_reply":"2024-05-03T15:03:11.550339Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"torch.Size([622, 1, 256, 256])"},"metadata":{}}]},{"cell_type":"code","source":"output=model(images_tensor[0].to(device))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:03:14.927267Z","iopub.execute_input":"2024-05-03T15:03:14.927661Z","iopub.status.idle":"2024-05-03T15:03:14.934801Z","shell.execute_reply.started":"2024-05-03T15:03:14.927629Z","shell.execute_reply":"2024-05-03T15:03:14.933637Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:03:18.957646Z","iopub.execute_input":"2024-05-03T15:03:18.958554Z","iopub.status.idle":"2024-05-03T15:03:18.965707Z","shell.execute_reply.started":"2024-05-03T15:03:18.958518Z","shell.execute_reply":"2024-05-03T15:03:18.964711Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0491, -0.0023, -0.0577, -0.0494, -0.0127, -0.0145, -0.0134, -0.0152]],\n       device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  for param in pretrained_resnet.parameters(): \n#         param.requires_grad = False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained_resnet.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output= model_2(images_tensor[0].to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_tensor[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output= model(images_tensor[0].to(device))\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_tensor[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train the model\n# num_epochs = 30\n# for epoch in range(num_epochs):\n#     running_loss = 0.0\n#     for images,landmarks in zip(image_dataloader,landmarks_dataloader):\n#         images, landmarks = images.to(device), landmarks.to(device)\n#         optimizer.zero_grad()\n#         outputs=pretrained_resnet(images)\n#         loss = criterion(outputs, landmarks)\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n\n#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(image_dataloader):.4f}')\n\n# # Save the trained model\n# torch.save(model.state_dict(), 'landmark_detection_model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer=torch.optim.Adam(model_2.parameters(),lr=0.002)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:09:21.365616Z","iopub.execute_input":"2024-05-03T15:09:21.366006Z","iopub.status.idle":"2024-05-03T15:09:21.371533Z","shell.execute_reply.started":"2024-05-03T15:09:21.365973Z","shell.execute_reply":"2024-05-03T15:09:21.370507Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 30\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for images,landmarks in zip(image_dataloader,landmarks_dataloader):\n        images, landmarks = images.to(device), landmarks.to(device)\n        optimizer.zero_grad()\n        outputs = model_2(images)\n        loss = criterion(outputs, landmarks)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(image_dataloader):.4f}')\n\n# Save the trained model\ntorch.save(model_2.state_dict(), 'landmark_detection_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:09:24.744236Z","iopub.execute_input":"2024-05-03T15:09:24.744607Z","iopub.status.idle":"2024-05-03T15:10:16.431080Z","shell.execute_reply.started":"2024-05-03T15:09:24.744576Z","shell.execute_reply":"2024-05-03T15:10:16.429899Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 210.9918\nEpoch [2/30], Loss: 104.3621\nEpoch [3/30], Loss: 99.7092\nEpoch [4/30], Loss: 98.4530\nEpoch [5/30], Loss: 96.0667\nEpoch [6/30], Loss: 96.7302\nEpoch [7/30], Loss: 92.8839\nEpoch [8/30], Loss: 92.3827\nEpoch [9/30], Loss: 92.7652\nEpoch [10/30], Loss: 91.4994\nEpoch [11/30], Loss: 92.2332\nEpoch [12/30], Loss: 90.9742\nEpoch [13/30], Loss: 90.4162\nEpoch [14/30], Loss: 86.0955\nEpoch [15/30], Loss: 82.6936\nEpoch [16/30], Loss: 83.3227\nEpoch [17/30], Loss: 81.5951\nEpoch [18/30], Loss: 81.8773\nEpoch [19/30], Loss: 79.7207\nEpoch [20/30], Loss: 81.4272\nEpoch [21/30], Loss: 81.4137\nEpoch [22/30], Loss: 81.7069\nEpoch [23/30], Loss: 80.8086\nEpoch [24/30], Loss: 80.3042\nEpoch [25/30], Loss: 77.1529\nEpoch [26/30], Loss: 76.9025\nEpoch [27/30], Loss: 77.6874\nEpoch [28/30], Loss: 79.7108\nEpoch [29/30], Loss: 76.0474\nEpoch [30/30], Loss: 77.3233\n","output_type":"stream"}]},{"cell_type":"code","source":"output_2= model_2(images_tensor[1].to(device))\noutput_2","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:11:09.161036Z","iopub.execute_input":"2024-05-03T15:11:09.161662Z","iopub.status.idle":"2024-05-03T15:11:09.171681Z","shell.execute_reply.started":"2024-05-03T15:11:09.161631Z","shell.execute_reply":"2024-05-03T15:11:09.170697Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"tensor([[389.4634, 134.6282, 388.3868, 448.7248, 171.1540, 260.8532, 619.5228,\n         269.7719]], device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"landmarks_tensor[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:12:00.199773Z","iopub.execute_input":"2024-05-03T15:12:00.200169Z","iopub.status.idle":"2024-05-03T15:12:00.207906Z","shell.execute_reply.started":"2024-05-03T15:12:00.200139Z","shell.execute_reply":"2024-05-03T15:12:00.206709Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"tensor([381.,  96., 390., 462., 132., 288., 633., 280.])"},"metadata":{}}]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 30\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for images,landmarks in zip(image_dataloader,landmarks_dataloader):\n        images, landmarks = images.to(device), landmarks.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, landmarks)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(image_dataloader):.4f}')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'landmark_detection_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:03:30.563015Z","iopub.execute_input":"2024-05-03T15:03:30.563927Z","iopub.status.idle":"2024-05-03T15:04:35.270645Z","shell.execute_reply.started":"2024-05-03T15:03:30.563891Z","shell.execute_reply":"2024-05-03T15:04:35.269613Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 117.4398\nEpoch [2/30], Loss: 86.1935\nEpoch [3/30], Loss: 82.7150\nEpoch [4/30], Loss: 86.1513\nEpoch [5/30], Loss: 80.8872\nEpoch [6/30], Loss: 78.3343\nEpoch [7/30], Loss: 83.8302\nEpoch [8/30], Loss: 68.9801\nEpoch [9/30], Loss: 78.8590\nEpoch [10/30], Loss: 71.4485\nEpoch [11/30], Loss: 59.4436\nEpoch [12/30], Loss: 55.6692\nEpoch [13/30], Loss: 57.1080\nEpoch [14/30], Loss: 52.6813\nEpoch [15/30], Loss: 58.6369\nEpoch [16/30], Loss: 58.8805\nEpoch [17/30], Loss: 69.5424\nEpoch [18/30], Loss: 71.5788\nEpoch [19/30], Loss: 72.6321\nEpoch [20/30], Loss: 62.7138\nEpoch [21/30], Loss: 56.9316\nEpoch [22/30], Loss: 51.2693\nEpoch [23/30], Loss: 47.6080\nEpoch [24/30], Loss: 52.2225\nEpoch [25/30], Loss: 54.1456\nEpoch [26/30], Loss: 30.6867\nEpoch [27/30], Loss: 29.3048\nEpoch [28/30], Loss: 45.7529\nEpoch [29/30], Loss: 49.0254\nEpoch [30/30], Loss: 41.8417\n","output_type":"stream"}]},{"cell_type":"code","source":"output= model(images_tensor[1].to(device))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:04:38.490774Z","iopub.execute_input":"2024-05-03T15:04:38.491669Z","iopub.status.idle":"2024-05-03T15:04:38.498338Z","shell.execute_reply.started":"2024-05-03T15:04:38.491622Z","shell.execute_reply":"2024-05-03T15:04:38.497216Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:04:42.993645Z","iopub.execute_input":"2024-05-03T15:04:42.994403Z","iopub.status.idle":"2024-05-03T15:04:43.001724Z","shell.execute_reply.started":"2024-05-03T15:04:42.994372Z","shell.execute_reply":"2024-05-03T15:04:43.000664Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"tensor([[327.9156,  72.1777, 321.1572, 371.9760, 136.6906, 232.7710, 533.1948,\n         226.3174]], device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"landmarks_tensor[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:04:49.153157Z","iopub.execute_input":"2024-05-03T15:04:49.153815Z","iopub.status.idle":"2024-05-03T15:04:49.161196Z","shell.execute_reply.started":"2024-05-03T15:04:49.153781Z","shell.execute_reply":"2024-05-03T15:04:49.160160Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"tensor([381.,  96., 390., 462., 132., 288., 633., 280.])"},"metadata":{}}]}]}